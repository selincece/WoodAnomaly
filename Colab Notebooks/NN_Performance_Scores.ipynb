{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cCL6mF_U_O8Ip312enqs916LsuZd6JU7","timestamp":1747395401377}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Test Results"],"metadata":{"id":"jHgcoOq6ChGb"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VUOISvtD7nw","executionInfo":{"status":"ok","timestamp":1747644378146,"user_tz":-180,"elapsed":21889,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}},"outputId":"c4b97286-d655-4cc7-82aa-3184ef8d4355"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install onnx onnxruntime onnxsim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKXOEdIiDvLy","executionInfo":{"status":"ok","timestamp":1747644403235,"user_tz":-180,"elapsed":22543,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}},"outputId":"107b7d0f-7eae-4fe1-e56d-efeb968f5ef6","collapsed":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Collecting onnxruntime\n","  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Collecting onnxsim\n","  Downloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n","Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n","Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from onnxsim) (13.9.4)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (2.19.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n","Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxsim, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.18.0 onnxruntime-1.22.0 onnxsim-0.4.36\n"]}]},{"cell_type":"markdown","source":["### Definitions"],"metadata":{"id":"DMupgWa9oNmy"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"2G9pBUNYCaSd","collapsed":true,"executionInfo":{"status":"ok","timestamp":1747644445364,"user_tz":-180,"elapsed":2091,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}}},"outputs":[],"source":["# Anomaly Detection Evaluation Notebook\n","\n","import os\n","import cv2\n","import numpy as np\n","import onnxruntime\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, jaccard_score\n","import random\n","import matplotlib.pyplot as plt\n","\n","# === Utility Functions ===\n","def load_mask(path, size=(256, 256)):\n","    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","    mask = cv2.resize(mask, size)\n","    return (mask > 127).astype(np.uint8)\n","\n","def threshold_heatmap(heatmap, threshold):\n","    return (heatmap >= threshold).astype(np.uint8)\n","\n","def compute_metrics(gt, pred):\n","    gt_flat = gt.flatten()\n","    pred_flat = pred.flatten()\n","    return f1_score(gt_flat, pred_flat), jaccard_score(gt_flat, pred_flat)\n","\n","def compute_image_level_f1(gt_mask, pred_mask):\n","    gt_label = int(np.any(gt_mask))\n","    pred_label = int(np.any(pred_mask))\n","    return f1_score([gt_label], [pred_label])\n","\n","def crop_dark_edges(image_np, darkness_threshold=30, strip_width=5):\n","    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n","    h, w = gray.shape\n","    left = 0\n","    right = w - 1\n","\n","    # Sum over small strips instead of individual columns to avoid single-pixel edges\n","    while left + strip_width < right:\n","        strip = gray[:, left:left + strip_width]\n","        if np.mean(strip) > darkness_threshold:\n","            break\n","        left += strip_width\n","\n","    while right - strip_width > left:\n","        strip = gray[:, right - strip_width:right]\n","        if np.mean(strip) > darkness_threshold:\n","            break\n","        right -= strip_width\n","\n","    return image_np[:, left:right+1]\n","\n","\n","# === INP-Former Inference ===\n","def run_inpformer(image, model_path, input_size=392):\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","\n","    resized = cv2.resize(image, (input_size, input_size))\n","    normed = (resized / 255.0 - mean) / std\n","    transposed = np.transpose(normed, (2, 0, 1))\n","    input_tensor = np.expand_dims(transposed, axis=0).astype(np.float32)\n","\n","    session = onnxruntime.InferenceSession(model_path)\n","    outputs = session.run(None, {session.get_inputs()[0].name: input_tensor})\n","    enc, dec = outputs[:2], outputs[2:4]\n","\n","    maps = []\n","    for e, d in zip(enc, dec):\n","        e, d = e[0], d[0]\n","        e, d = np.transpose(e, (1, 2, 0)), np.transpose(d, (1, 2, 0))\n","        sim = 1 - np.sum(e * d, axis=2) / (np.linalg.norm(e, axis=2) * np.linalg.norm(d, axis=2) + 1e-8)\n","        sim = cv2.resize(sim, (256, 256))\n","        maps.append(sim)\n","\n","    anomaly_map = np.mean(maps, axis=0)\n","    anomaly_map = cv2.GaussianBlur(anomaly_map, (5, 5), sigmaX=4)\n","    return anomaly_map\n","\n","\n","# === Load Random Test Samples ===\n","def collect_random_samples(dataset_root, max_samples=10):\n","    test_path = os.path.join(dataset_root, \"test\")\n","    gt_path = os.path.join(dataset_root, \"ground_truth\")\n","\n","    image_mask_pairs = []\n","\n","    for defect_type in os.listdir(test_path):\n","        if defect_type == \"good\":\n","            continue\n","        defect_folder = os.path.join(test_path, defect_type)\n","        gt_folder = os.path.join(gt_path, defect_type)\n","\n","        for filename in os.listdir(defect_folder):\n","            if filename.endswith(\".jpg\"):\n","                image_path = os.path.join(defect_folder, filename)\n","                mask_path = os.path.join(gt_folder, filename.replace(\".jpg\", \"_mask.jpg\"))\n","                if os.path.exists(mask_path):\n","                    image_mask_pairs.append((image_path, mask_path))\n","\n","    return random.sample(image_mask_pairs, min(max_samples, len(image_mask_pairs)))\n","\n","\n","# === Paths ===\n","dataset_root = \"/content/drive/MyDrive/Neural_Networks_Project/wood_dataset/wood\""]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import onnxruntime\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, jaccard_score\n","import random\n","import matplotlib.pyplot as plt\n","import sys\n","import torch\n","\n","\n","def run_inpformer(image, model_path, input_size=392):\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","\n","    resized = cv2.resize(image, (input_size, input_size))\n","    normed = (resized / 255.0 - mean) / std\n","    transposed = np.transpose(normed, (2, 0, 1))\n","    input_tensor = np.expand_dims(transposed, axis=0).astype(np.float32)\n","\n","    session = onnxruntime.InferenceSession(model_path)\n","    outputs = session.run(None, {session.get_inputs()[0].name: input_tensor})\n","    enc, dec = outputs[:2], outputs[2:4]\n","\n","    maps = []\n","    for e, d in zip(enc, dec):\n","        e, d = e[0], d[0]\n","        e, d = np.transpose(e, (1, 2, 0)), np.transpose(d, (1, 2, 0))\n","        sim = 1 - np.sum(e * d, axis=2) / (np.linalg.norm(e, axis=2) * np.linalg.norm(d, axis=2) + 1e-8)\n","        sim = cv2.resize(sim, (256, 256))\n","        maps.append(sim)\n","\n","    anomaly_map = np.mean(maps, axis=0)\n","    anomaly_map = cv2.GaussianBlur(anomaly_map, (5, 5), sigmaX=4)\n","    return anomaly_map\n","\n","\n","def load_efficientad_model(model_path):\n","    sys.path.append('/content/drive/MyDrive/Neural_Networks_Project/EfficientAD')\n","    from common import get_pdn_small, get_autoencoder\n","    from efficientad import teacher_normalization, map_normalization, predict\n","    from torch.serialization import add_safe_globals\n","    import torch.nn as nn\n","    from torchvision import transforms as T\n","\n","    add_safe_globals({\"Sequential\": nn.Sequential})\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    weights_dir = model_path\n","\n","    teacher = torch.load(f\"{weights_dir}/teacher_final.pth\", map_location=device, weights_only=False)\n","    student = torch.load(f\"{weights_dir}/student_final.pth\", map_location=device, weights_only=False)\n","    autoencoder = torch.load(f\"{weights_dir}/autoencoder_final.pth\", map_location=device, weights_only=False)\n","\n","    teacher.to(device).eval()\n","    student.to(device).eval()\n","    autoencoder.to(device).eval()\n","\n","    transform = T.Compose([\n","        T.Resize((256, 256)),\n","        T.ToTensor(),\n","        T.Normalize(mean=[0.485, 0.456, 0.406],\n","                    std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    return teacher, student, autoencoder, predict, 0.5, 0.1, 0.05, 0.95, 0.05, 0.95, transform, device\n","\n","\n","def run_efficientad(image, model_objs):\n","    from PIL import Image as PILImage\n","    (teacher, student, autoencoder, predict, teacher_mean, teacher_std,\n","     q_st_start, q_st_end, q_ae_start, q_ae_end, transform, device) = model_objs\n","\n","    image_pil = PILImage.fromarray(image)\n","    img_tensor = transform(image_pil).unsqueeze(0).to(device)\n","\n","    map_combined, _, _ = predict(img_tensor, teacher, student, autoencoder,\n","                                 teacher_mean, teacher_std,\n","                                 q_st_start, q_st_end, q_ae_start, q_ae_end)\n","\n","    heatmap = map_combined.squeeze().cpu().numpy()\n","    return 1.0 - heatmap\n","\n","def run_glass(image_batch, model_path, input_size=(256, 256), expected_batch_size=8):\n","    from torchvision import transforms\n","    from PIL import Image as PILImage\n","\n","    transform = transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # Convert list of images to a batch tensor\n","    img_tensors = []\n","    for image in image_batch: # 'image_batch' is the input argument here\n","         image_pil = PILImage.fromarray(image)\n","         img_tensors.append(transform(image_pil).unsqueeze(0))\n","\n","    # Stack tensors to create the batch\n","    # Ensure tensors are on the same device if using GPU, though ONNX Runtime handles device\n","    img_batch_tensor = torch.cat(img_tensors, dim=0).numpy()\n","\n","    # Initialize ONNX Runtime session\n","    session = onnxruntime.InferenceSession(model_path)\n","\n","    # Run inference\n","    # The input name 'input' is from the traceback. Verify with the actual model if needed.\n","    # The model expects a batch size of 8. The batch_tensor is explicitly created with\n","    # the size of the list of images (padded to 8 in get_global_heatmap_stats).\n","    outputs = session.run(None, {\"input\": img_batch_tensor})\n","    # The output is expected to be a batch of heatmaps (B, H, W) or (B, 1, H, W).\n","    # Assuming (B, H, W) or squeezing the channel dimension if it exists.\n","    heatmaps = outputs[0] # outputs is a list, outputs[0] is the tensor\n","\n","    return heatmaps # Return batch of heatmaps\n","\n","\n"],"metadata":{"id":"murZT5C4ZtjI","executionInfo":{"status":"ok","timestamp":1747644450134,"user_tz":-180,"elapsed":4765,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import jaccard_score\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import random\n","from sklearn.metrics import f1_score # Ensure f1_score is imported if not already\n","\n","\n","def evaluate_image_level(dataset_root, model_name, model_path, threshold=0.5, sample_count=15, topk_percent=0.007,\n","                         global_min=0.0, global_max=0.5):\n","    test_path = os.path.join(dataset_root, \"test\")\n","    gt_path = os.path.join(dataset_root, \"ground_truth\")\n","    image_label_pairs = []\n","\n","    for defect_type in os.listdir(test_path):\n","        defect_folder = os.path.join(test_path, defect_type)\n","        is_defective = defect_type != \"good\"\n","        gt_folder = os.path.join(gt_path, defect_type) if is_defective else None\n","\n","        for filename in os.listdir(defect_folder):\n","            if filename.endswith(\".jpg\"):\n","                image_path = os.path.join(defect_folder, filename)\n","                mask_path = os.path.join(gt_folder, filename.replace(\".jpg\", \"_mask.jpg\")) if is_defective else None\n","                # Store image_path, true_label, mask_path for later retrieval\n","                image_label_pairs.append((image_path, int(is_defective), mask_path))\n","\n","    samples = random.sample(image_label_pairs, min(sample_count, len(image_label_pairs)))\n","\n","    if model_name == \"EfficientAD\":\n","        efficientad_model = load_efficientad_model(model_path)\n","    else:\n","        efficientad_model = None # Ensure this is None if not EfficientAD\n","\n","    y_true = []\n","    y_pred = []\n","    processed_samples_info = [] # Store info to process after batch inference for GLASS\n","\n","    # --- Batch processing for GLASS model ---\n","    if model_name == \"GLASS\":\n","        expected_batch_size = 8 # Fixed batch size for the GLASS model\n","        current_batch_images_rgb_cropped_resized = [] # Store cropped and resized RGB images for batch input\n","        current_batch_info = [] # Store (true_label, mask_path, cropped_image_resized_for_vis) for this batch\n","\n","        for i, (image_path, true_label, mask_path) in enumerate(tqdm(samples, desc=f\"Processing {model_name} samples (Batching)\")):\n","            image = cv2.imread(image_path)\n","            if image is None:\n","                print(f\"Warning: Could not load image from {image_path}. Skipping.\")\n","                continue\n","\n","            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            # Apply cropping here\n","            cropped_image_rgb = crop_dark_edges(image_rgb)\n","\n","            # Resize the *cropped* image for model input and visualization\n","            # The run_glass function resizes to (256, 256) internally,\n","            # but we need a consistent size *after* cropping for batching\n","            # and for generating a visualization image that matches heatmap dimensions.\n","            # Let's resize the cropped image to the expected input size (e.g. 256x256) for the batch.\n","            # We'll also use this as the image for visualization.\n","            cropped_image_resized_for_vis = cv2.resize(cropped_image_rgb, (256, 256))\n","\n","            current_batch_images_rgb_cropped_resized.append(cropped_image_resized_for_vis) # Add cropped and resized image to batch\n","            current_batch_info.append((true_label, mask_path, cropped_image_resized_for_vis)) # Store labels/paths and the image used for vis\n","\n","            # If batch is full or it's the last sample\n","            if len(current_batch_images_rgb_cropped_resized) == expected_batch_size or i == len(samples) - 1:\n","                original_batch_size = len(current_batch_images_rgb_cropped_resized)\n","                # Pad the batch with dummy images (black image of the expected input size)\n","                dummy_image = np.zeros((256, 256, 3), dtype=np.uint8) # Match expected input dims (256, 256, 3)\n","\n","                while len(current_batch_images_rgb_cropped_resized) < expected_batch_size:\n","                     current_batch_images_rgb_cropped_resized.append(dummy_image)\n","\n","\n","                try:\n","                    # Pass the batch of cropped and resized images to run_glass\n","                    # run_glass will perform its own internal resizing if needed, but\n","                    # providing it with images already at a standard size like 256x256\n","                    # can sometimes align better with how models were trained/exported.\n","                    # Assuming the GLASS model's internal resize handles this correctly.\n","                    heatmap_batch = run_glass(current_batch_images_rgb_cropped_resized, model_path, input_size=(256, 256))\n","\n","\n","                    # Process heatmaps for the *real* images in the batch\n","                    for j in range(original_batch_size):\n","                        heatmap = heatmap_batch[j] # Get heatmap for the j-th image in the current batch\n","                        true_label, mask_path, image_resized_for_vis = current_batch_info[j] # Get the corresponding info (including cropped/resized image)\n","\n","                        # Ensure heatmap is 2D before flattening and processing\n","                        if heatmap.ndim > 2:\n","                             heatmap = np.squeeze(heatmap)\n","                             if heatmap.ndim > 2:\n","                                  print(f\"Warning: Heatmap still has unexpected dimensions ({heatmap.ndim}) after squeeze. Taking first channel slice.\")\n","                                  # Fallback to slicing if squeeze wasn't enough, assuming channel is last dim\n","                                  heatmap = heatmap[:, :, 0] if heatmap.shape[-1] > 1 else heatmap.squeeze()\n","\n","\n","                        # The heatmap output should ideally match the input size (256, 256).\n","                        # Verify and resize if necessary for visualization/metrics.\n","                        if heatmap.shape[:2] != (256, 256):\n","                            print(f\"Warning: Heatmap has shape {heatmap.shape}. Resizing to (256, 256) for visualization.\")\n","                            heatmap = cv2.resize(heatmap, (256, 256), interpolation=cv2.INTER_LINEAR) # Use linear for heatmaps\n","\n","\n","                        processed_samples_info.append({\n","                            'image_resized_for_vis': image_resized_for_vis, # This is now the cropped and resized image\n","                            'heatmap': heatmap,\n","                            'true_label': true_label,\n","                            'mask_path': mask_path,\n","                            'image_path': samples[i - original_batch_size + j][0] # Get original path for logging\n","                        })\n","\n","                except Exception as e:\n","                    # Get the path of the first image in the problematic batch for logging\n","                    first_image_path_in_batch = samples[i - original_batch_size + (0 if original_batch_size > 0 else 0)][0]\n","                    print(f\"Error processing GLASS batch starting with image index {i - original_batch_size + 1} ({first_image_path_in_batch}): {e}\")\n","                    # Continue to the next batch\n","\n","                # Reset the batch lists\n","                current_batch_images_rgb_cropped_resized = []\n","                current_batch_info = []\n","\n","        # Now iterate through processed_samples_info for visualization and metrics\n","        print(\"\\nProcessing individual samples for visualization and metrics...\")\n","        for sample_info in tqdm(processed_samples_info, desc=\"Generating visualizations\"):\n","            image_resized_for_vis = sample_info['image_resized_for_vis'] # This is the cropped and resized image\n","            heatmap = sample_info['heatmap']\n","            true_label = sample_info['true_label']\n","            mask_path = sample_info['mask_path']\n","            image_path = sample_info['image_path'] # Use for logging\n","\n","            flat = heatmap.flatten()\n","            if len(flat) == 0:\n","                 print(f\"Warning: Heatmap for {image_path} is empty. Skipping metrics and visualization.\")\n","                 continue\n","\n","            k = max(1, int(len(flat) * topk_percent))\n","            k = min(k, len(flat)) # Ensure k is within bounds\n","            if k > 0:\n","                 topk_mean = np.mean(np.partition(flat, -k)[-k:])\n","            else:\n","                 topk_mean = 0.0\n","\n","            pred_label = int(topk_mean > threshold)\n","\n","            y_true.append(true_label)\n","            y_pred.append(pred_label)\n","\n","            # Visualization using global min/max\n","            range_val = global_max - global_min\n","            if range_val <= 1e-8:\n","                 print(f\"Warning: Global min ({global_min}) and max ({global_max}) are too close. Using default range [0, 1] for visualization stretching for {image_path}.\")\n","                 heatmap_vis = np.clip(heatmap, 0, 1)\n","            else:\n","                 heatmap_vis = (heatmap - global_min) / range_val\n","                 heatmap_vis = np.clip(heatmap_vis, 0, 1)\n","\n","            heatmap_color = cv2.applyColorMap((heatmap_vis * 255).astype(np.uint8), cv2.COLORMAP_JET)\n","            heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n","\n","            # The heatmap is already resized to (256, 256).\n","            # The image_resized_for_vis is also (256, 256).\n","            # No resizing needed for the heatmap_color before blending if dimensions match.\n","            if heatmap_color.shape[:2] != image_resized_for_vis.shape[:2]:\n","                 print(f\"Warning: Heatmap color shape {heatmap_color.shape[:2]} does not match visualization image shape {image_resized_for_vis.shape[:2]}. Resizing heatmap_color for overlay.\")\n","                 heatmap_color_resized = cv2.resize(heatmap_color, (image_resized_for_vis.shape[1], image_resized_for_vis.shape[0]))\n","            else:\n","                 heatmap_color_resized = heatmap_color\n","\n","\n","            # Ensure dtypes are uint8 for blending\n","            if image_resized_for_vis.dtype != np.uint8:\n","                 image_resized_for_vis = image_resized_for_vis.astype(np.uint8)\n","            if heatmap_color_resized.dtype != np.uint8:\n","                 heatmap_color_resized = heatmap_color_resized.astype(np.uint8)\n","\n","\n","            overlay = cv2.addWeighted(image_resized_for_vis, 0.6, heatmap_color_resized, 0.4, 0)\n","\n","            # Plotting\n","            # Note: Ground truth mask is loaded at size (256, 256).\n","            # The predicted mask for IoU calculation should also be (256, 256).\n","            # Since heatmap is already resized to (256, 256), this is fine.\n","            if true_label == 1 and mask_path:\n","                gt_mask = load_mask(mask_path, size=(256, 256)) # Ensure mask is 256x256\n","                # Use the already resized heatmap (256, 256) for thresholding\n","                pred_mask_thresholded = (heatmap > threshold).astype(np.uint8)\n","\n","                if gt_mask is not None and gt_mask.shape == pred_mask_thresholded.shape:\n","                     iou = jaccard_score(gt_mask.flatten(), pred_mask_thresholded.flatten())\n","                     iou_title = f\"IoU={iou:.2f}\"\n","                else:\n","                     iou_title = \"Mask not loaded or size mismatch\"\n","                     if gt_mask is not None and gt_mask.shape != pred_mask_thresholded.shape:\n","                          print(f\"Warning: Ground truth mask shape {gt_mask.shape} does not match predicted mask shape {pred_mask_thresholded.shape} for {mask_path}. Cannot compute IoU.\")\n","                          gt_mask = None\n","\n","                fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n","                ax[0].imshow(image_resized_for_vis) # Now plots the cropped and resized image\n","                ax[0].set_title(f\"Image: {'Defect' if true_label else 'Good'}\")\n","                ax[0].axis(\"off\")\n","\n","                ax[1].imshow(overlay) # Overlay is on the cropped and resized image\n","                ax[1].set_title(f\"Prediction: {'Defect' if pred_label else 'Good'} (score={topk_mean:.4f})\")\n","                ax[1].axis(\"off\")\n","\n","                if gt_mask is not None:\n","                     ax[2].imshow(gt_mask, cmap=\"gray\", vmin=0, vmax=1)\n","                     ax[2].set_title(f\"Ground Truth\\n{iou_title}\")\n","                     ax[2].axis(\"off\")\n","                else:\n","                     ax[2].set_title(f\"Ground Truth Mask Missing\\n{iou_title}\")\n","                     ax[2].axis(\"off\")\n","\n","                plt.tight_layout()\n","                plt.show()\n","            else:\n","                fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n","                ax[0].imshow(image_resized_for_vis) # Now plots the cropped and resized image\n","                ax[0].set_title(f\"Image: {'Defect' if true_label else 'Good'}\")\n","                ax[0].axis(\"off\")\n","\n","                ax[1].imshow(overlay) # Overlay is on the cropped and resized image\n","                ax[1].set_title(f\"Prediction: {'Defect' if pred_label else 'Good'} (score={topk_mean:.4f})\")\n","                ax[1].axis(\"off\")\n","                plt.tight_layout()\n","                plt.show()\n","\n","\n","    # --- Single image processing for other models (INP-Former, EfficientAD) ---\n","    else:\n","        for image_path, true_label, mask_path in tqdm(samples, desc=f\"Processing {model_name} samples\"):\n","            image = cv2.imread(image_path)\n","            if image is None:\n","                print(f\"Warning: Could not load image from {image_path}. Skipping.\")\n","                continue\n","\n","            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            # Apply cropping here\n","            cropped_image_rgb = crop_dark_edges(image_rgb)\n","\n","            # Resize the *cropped* image for visualization before processing\n","            image_resized_for_vis = cv2.resize(cropped_image_rgb, (256, 256)) # Resize the cropped image\n","\n","\n","            # Pass the cropped image to the model function\n","            # Note: run_inpformer and run_efficientad have their own internal resizing logic.\n","            # Passing the *cropped* image is correct, they will handle their specific input size.\n","            if model_name == \"INP-Former\":\n","                heatmap = run_inpformer(cropped_image_rgb, model_path) # Pass cropped image\n","            elif model_name == \"EfficientAD\":\n","                 heatmap = run_efficientad(cropped_image_rgb, efficientad_model) # Pass cropped image\n","            else:\n","                # This case is handled by the batching logic above, but kept for clarity\n","                raise ValueError(f\"Unsupported model: {model_name}\")\n","\n","            # Ensure heatmap is 2D before flattening and processing\n","            if heatmap.ndim > 2:\n","                 heatmap = np.squeeze(heatmap)\n","                 if heatmap.ndim > 2:\n","                      print(f\"Warning: Heatmap for {image_path} still has unexpected dimensions ({heatmap.ndim}) after squeeze. Taking first channel slice.\")\n","                      # Fallback to slicing if squeeze wasn't enough, assuming channel is last dim\n","                      heatmap = heatmap[:, :, 0] if heatmap.shape[-1] > 1 else heatmap.squeeze()\n","\n","\n","            # Resize heatmap to match visualization size (256, 256)\n","            # For INP-Former/EfficientAD, the output is expected at 256x256.\n","            # We still perform a check and resize here for safety.\n","            if heatmap.shape[:2] != (256, 256):\n","                 print(f\"Warning: Heatmap for {image_path} has shape {heatmap.shape}. Resizing to (256, 256) for visualization.\")\n","                 heatmap = cv2.resize(heatmap, (256, 256), interpolation=cv2.INTER_LINEAR)\n","\n","\n","            flat = heatmap.flatten()\n","            if len(flat) == 0:\n","                 print(f\"Warning: Heatmap for {image_path} is empty after processing. Skipping metrics and visualization for this image.\")\n","                 continue\n","\n","\n","            k = max(1, int(len(flat) * topk_percent))\n","            k = min(k, len(flat)) # Ensure k is within bounds\n","            if k > 0:\n","                topk_mean = np.mean(np.partition(flat, -k)[-k:])\n","            else:\n","                topk_mean = 0.0\n","\n","            pred_label = int(topk_mean > threshold)\n","\n","            y_true.append(true_label)\n","            y_pred.append(pred_label)\n","\n","            # Visualization using global min/max\n","            range_val = global_max - global_min\n","            if range_val <= 1e-8:\n","                 print(f\"Warning: Global min ({global_min}) and max ({global_max}) are too close. Using default range [0, 1] for visualization stretching for {image_path}.\")\n","                 heatmap_vis = np.clip(heatmap, 0, 1)\n","            else:\n","                 heatmap_vis = (heatmap - global_min) / range_val\n","                 heatmap_vis = np.clip(heatmap_vis, 0, 1)\n","\n","\n","            heatmap_color = cv2.applyColorMap((heatmap_vis * 255).astype(np.uint8), cv2.COLORMAP_JET)\n","            heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n","\n","            # The heatmap_color is already (256, 256, 3).\n","            # The image_resized_for_vis is also (256, 256, 3).\n","            # No resizing needed for the heatmap_color before blending if dimensions match.\n","            if heatmap_color.shape[:2] != image_resized_for_vis.shape[:2]:\n","                 print(f\"Warning: Heatmap color shape {heatmap_color.shape[:2]} does not match visualization image shape {image_resized_for_vis.shape[:2]}. Resizing heatmap_color for overlay.\")\n","                 heatmap_color_resized = cv2.resize(heatmap_color, (image_resized_for_vis.shape[1], image_resized_for_vis.shape[0]))\n","            else:\n","                 heatmap_color_resized = heatmap_color\n","\n","            # Ensure dtypes are uint8 for blending\n","            if image_resized_for_vis.dtype != np.uint8:\n","                 image_resized_for_vis = image_resized_for_vis.astype(np.uint8)\n","            if heatmap_color_resized.dtype != np.uint8:\n","                 heatmap_color_resized = heatmap_color_resized.astype(np.uint8)\n","\n","\n","            overlay = cv2.addWeighted(image_resized_for_vis, 0.6, heatmap_color_resized, 0.4, 0)\n","\n","            # Plotting\n","            # Note: Ground truth mask is loaded at size (256, 256).\n","            # The predicted mask for IoU calculation should also be (256, 256).\n","            # Since heatmap is already resized to (256, 256), this is fine.\n","            if true_label == 1 and mask_path:\n","                gt_mask = load_mask(mask_path, size=(256, 256)) # Ensure mask is 256x256\n","                # Use the already resized heatmap (256, 256) for thresholding\n","                pred_mask_thresholded = (heatmap > threshold).astype(np.uint8)\n","\n","                if gt_mask is not None and gt_mask.shape == pred_mask_thresholded.shape:\n","                     iou = jaccard_score(gt_mask.flatten(), pred_mask_thresholded.flatten())\n","                     iou_title = f\"IoU={iou:.2f}\"\n","                else:\n","                     iou_title = \"Mask not loaded or size mismatch\"\n","                     if gt_mask is not None and gt_mask.shape != pred_mask_thresholded.shape:\n","                          print(f\"Warning: Ground truth mask shape {gt_mask.shape} does not match predicted mask shape {pred_mask_thresholded.shape} for {mask_path}. Cannot compute IoU.\")\n","                          gt_mask = None\n","\n","\n","                fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n","                ax[0].imshow(image_resized_for_vis) # Now plots the cropped and resized image\n","                ax[0].set_title(f\"Image: {'Defect' if true_label else 'Good'}\")\n","                ax[0].axis(\"off\")\n","\n","                ax[1].imshow(overlay) # Overlay is on the cropped and resized image\n","                ax[1].set_title(f\"Prediction: {'Defect' if pred_label else 'Good'} (score={topk_mean:.4f})\")\n","                ax[1].axis(\"off\")\n","\n","                if gt_mask is not None:\n","                     ax[2].imshow(gt_mask, cmap=\"gray\", vmin=0, vmax=1)\n","                     ax[2].set_title(f\"Ground Truth\\n{iou_title}\")\n","                     ax[2].axis(\"off\")\n","                else:\n","                     ax[2].set_title(f\"Ground Truth Mask Missing\\n{iou_title}\")\n","                     ax[2].axis(\"off\")\n","\n","                plt.tight_layout()\n","                plt.show()\n","            else:\n","                fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n","                ax[0].imshow(image_resized_for_vis) # Now plots the cropped and resized image\n","                ax[0].set_title(f\"Image: {'Defect' if true_label else 'Good'}\")\n","                ax[0].axis(\"off\")\n","\n","                ax[1].imshow(overlay) # Overlay is on the cropped and resized image\n","                ax[1].set_title(f\"Prediction: {'Defect' if pred_label else 'Good'} (score={topk_mean:.4f})\")\n","                ax[1].axis(\"off\")\n","                plt.tight_layout()\n","                plt.show()\n","\n","\n","    # Compute final F1 score\n","    if not y_true:\n","        print(\"\\n=== Image-level F1 Score ===\")\n","        print(\"No samples were processed. F1 Score cannot be computed.\")\n","        return\n","\n","    if len(y_true) != len(y_pred):\n","        print(f\"Warning: Mismatch between number of true labels ({len(y_true)}) and predicted labels ({len(y_pred)}). This should not happen.\")\n","        min_len = min(len(y_true), len(y_pred))\n","        y_true = y_true[:min_len]\n","        y_pred = y_pred[:min_len]\n","        if min_len == 0:\n","             print(\"No valid label pairs to compute F1 score.\")\n","             return\n","\n","\n","    score = f1_score(y_true, y_pred)\n","    print(\"\\n=== Image-level F1 Score ===\")\n","    print(f\"F1 Score: {score:.4f}\")"],"metadata":{"id":"SM6XSgL9Q3MG","executionInfo":{"status":"ok","timestamp":1747645450231,"user_tz":-180,"elapsed":67,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def get_global_heatmap_stats(dataset_root, model_name, model_path, sample_count=10):\n","    samples = collect_random_samples(dataset_root, max_samples=sample_count)\n","\n","    if not samples:\n","        print(\"Warning: No defective samples found or processed. Returning default stats.\")\n","        return 0.0, 1.0\n","\n","    all_heatmaps = []\n","\n","    if model_name == \"EfficientAD\":\n","        model_objs = load_efficientad_model(model_path)\n","        for image_path, mask_path in tqdm(samples, desc=f\"Processing {model_name} samples\"):\n","             image = cv2.imread(image_path)\n","             if image is None:\n","                 print(f\"Warning: Could not load image from {image_path}. Skipping.\")\n","                 continue\n","             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","             image = crop_dark_edges(image)\n","             heatmap = run_efficientad(image, model_objs)\n","             # Ensure heatmap is 2D before flattening\n","             if heatmap.ndim > 2:\n","                  heatmap = heatmap.squeeze()\n","             all_heatmaps.append(heatmap.flatten())\n","\n","    elif model_name == \"INP-Former\":\n","         for image_path, mask_path in tqdm(samples, desc=f\"Processing {model_name} samples\"):\n","             image = cv2.imread(image_path)\n","             if image is None:\n","                 print(f\"Warning: Could not load image from {image_path}. Skipping.\")\n","                 continue\n","             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","             image = crop_dark_edges(image)\n","             heatmap = run_inpformer(image, model_path)\n","             # Ensure heatmap is 2D before flattening\n","             if heatmap.ndim > 2:\n","                  heatmap = heatmap.squeeze()\n","             all_heatmaps.append(heatmap.flatten())\n","\n","    elif model_name == \"GLASS\":\n","        # For GLASS, we process in batches of 8 as required by the model.\n","        expected_batch_size = 8\n","        current_batch_images = []\n","        # Need to store original indices to correctly extract heatmaps later\n","        current_batch_original_indices = []\n","\n","        for i, (image_path, mask_path) in enumerate(tqdm(samples, desc=f\"Processing {model_name} samples\")):\n","            image = cv2.imread(image_path)\n","            if image is None:\n","                print(f\"Warning: Could not load image from {image_path}. Skipping.\")\n","                continue\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            image = crop_dark_edges(image)\n","            current_batch_images.append(image)\n","            current_batch_original_indices.append(i)\n","\n","            # If we have a full batch or it's the last image\n","            if len(current_batch_images) == expected_batch_size or i == len(samples) - 1:\n","                original_batch_size = len(current_batch_images)\n","                # Pad the batch if it's not full\n","                while len(current_batch_images) < expected_batch_size:\n","                    # Add a dummy image (e.g., black image) to fill the batch\n","                    # Use the shape of the first image in the current batch for consistency\n","                    if current_batch_images: # Ensure there's at least one real image\n","                        dummy_image = np.zeros_like(current_batch_images[0])\n","                    else: # Should not happen if samples is not empty, but as a fallback\n","                         dummy_image = np.zeros((256, 256, 3), dtype=np.uint8) # Default size\n","                    current_batch_images.append(dummy_image)\n","\n","                try:\n","                     # run_glass now expects a list of images, potentially padded\n","                     heatmap_batch = run_glass(current_batch_images, model_path)\n","\n","                     # Extract heatmaps for the *real* images in the batch\n","                     for j in range(original_batch_size):\n","                         heatmap = heatmap_batch[j] # Get heatmap for the j-th image in the current batch\n","                         # Ensure heatmap is 2D before flattening\n","                         if heatmap.ndim > 2:\n","                              heatmap = heatmap.squeeze()\n","                         all_heatmaps.append(heatmap.flatten())\n","\n","                except Exception as e:\n","                    # Get the path of the first image in the problematic batch for logging\n","                    first_image_path_in_batch = samples[current_batch_original_indices[0]][0]\n","                    print(f\"Error processing batch starting with image index {current_batch_original_indices[0]} ({first_image_path_in_batch}): {e}\")\n","                    # Continue to the next batch\n","\n","                # Reset the batch\n","                current_batch_images = []\n","                current_batch_original_indices = []\n","\n","    else:\n","        raise ValueError(f\"Unsupported model: {model_name}\")\n","\n","    # Calculate global min/max from all collected heatmaps\n","    if not all_heatmaps:\n","        print(\"Warning: No heatmaps were generated. Returning default stats.\")\n","        return 0.0, 1.0\n","\n","    all_heatmaps_flat = np.concatenate(all_heatmaps)\n","    global_min = np.min(all_heatmaps_flat)\n","    global_max = np.max(all_heatmaps_flat)\n","\n","    return global_min, global_max"],"metadata":{"id":"aQ-khx5JbhFz","executionInfo":{"status":"ok","timestamp":1747644450363,"user_tz":-180,"elapsed":122,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Limits"],"metadata":{"id":"bbMCcc2HoJ2d"}},{"cell_type":"code","source":["inpformer_onnx_path = \"/content/drive/MyDrive/Neural_Networks_Project/INP-Former/inpformer.onnx\"\n","global_min, global_max = get_global_heatmap_stats(dataset_root, \"INP-Former\", inpformer_onnx_path)\n","\n","print(f\"Global Min: {global_min}\")\n","print(f\"Global Max: {global_max}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06Xjum-Ybib8","executionInfo":{"status":"ok","timestamp":1747396880676,"user_tz":-180,"elapsed":88716,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}},"outputId":"ea4f27ee-532a-41a9-fcc8-44a197c2eb20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing INP-Former samples: 100%|██████████| 10/10 [01:27<00:00,  8.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Global Min: 0.06759029626846313\n","Global Max: 0.5027773380279541\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["efficientad_weights_folder = \"/content/drive/MyDrive/Neural_Networks_Project/EfficientAD/weights\"\n","global_min, global_max = get_global_heatmap_stats(dataset_root, \"EfficientAD\", efficientad_weights_folder)\n","\n","print(f\"Global Min: {global_min}\")\n","print(f\"Global Max: {global_max}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rnt6sPzqfQJ7","executionInfo":{"status":"ok","timestamp":1747389130315,"user_tz":-180,"elapsed":186761,"user":{"displayName":"Yunus D","userId":"04657504895391039056"}},"outputId":"075077c8-2aa8-4c58-c68c-0d219a5c3a7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [03:06<00:00, 18.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Global Min: -4.71906042098999\n","Global Max: -1.2570910453796387\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["glass_onnx_path = \"/content/drive/MyDrive/Neural_Networks_Project/glass.onnx\"\n","global_min, global_max = get_global_heatmap_stats(dataset_root, \"GLASS\", glass_onnx_path)\n","\n","print(f\"Global Min: {global_min}\")\n","print(f\"Global Max: {global_max}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8MgScRkTgIdu","executionInfo":{"status":"ok","timestamp":1747390031066,"user_tz":-180,"elapsed":25499,"user":{"displayName":"Yunus D","userId":"04657504895391039056"}},"outputId":"1515df39-7ed7-405a-faba-20193fcd9af7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing GLASS samples: 100%|██████████| 10/10 [00:25<00:00,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Global Min: 0.1804734468460083\n","Global Max: 0.9709413051605225\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Results"],"metadata":{"id":"q-8jQ2fEoF2P"}},{"cell_type":"code","source":["inpformer_onnx_path = \"/content/drive/MyDrive/Neural_Networks_Project/INP-Former/inpformer.onnx\"\n","dataset_root = \"/content/drive/MyDrive/Neural_Networks_Project/wood_dataset/wood\"\n","evaluate_image_level(dataset_root, \"INP-Former\", inpformer_onnx_path, threshold=0.29, global_min = 0.0, global_max = 0.5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1kbjKVVlGjLFmeIPJsuNWqG8qn9E4ky-Y"},"id":"a7Zn6D0vRHLr","executionInfo":{"status":"ok","timestamp":1747646654310,"user_tz":-180,"elapsed":99223,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}},"outputId":"47436ad0-54b4-48b6-e036-fcf177ac2dbb"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["efficientad_weights_folder = \"/content/drive/MyDrive/Neural_Networks_Project/EfficientAD/weights\"\n","dataset_root = \"/content/drive/MyDrive/Neural_Networks_Project/wood_dataset/wood\"\n","evaluate_image_level(dataset_root, \"EfficientAD\", efficientad_weights_folder, threshold= -1.9 , global_min = -4.75, global_max = -1.25)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1FmhGOacQ4lahnv4yB5AfICe8Snb2FqVq"},"id":"xisHjLvlfRDM","executionInfo":{"status":"ok","timestamp":1747647294619,"user_tz":-180,"elapsed":270908,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}},"outputId":"3a37b007-6e57-4017-d10d-e8184b0c4350"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["glass_onnx_path = \"/content/drive/MyDrive/Neural_Networks_Project/glass.onnx\"\n","dataset_root = \"/content/drive/MyDrive/Neural_Networks_Project/wood_dataset/wood\"\n","evaluate_image_level(dataset_root, \"GLASS\", glass_onnx_path, threshold= 0.7 , global_min = 0.15, global_max = 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1oMIqQgYKSqBpxWBRloX2-gDTFjH_JgtF"},"id":"YBPqgfi_fsLl","executionInfo":{"status":"ok","timestamp":1747645957814,"user_tz":-180,"elapsed":54256,"user":{"displayName":"Selin İnce","userId":"11388985702354756671"}},"outputId":"3388f102-6645-40d0-e2dd-0ffa6c782c11"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}